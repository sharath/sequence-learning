\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[backend=biber,sorting=none]{biblatex}
%\usepackage{geometry}
\usepackage{hyperref}
\usepackage{float}
\addbibresource{mybib.tex}

\title{Dynamical Neural Network Models\\for Sequence Learning}
\author{Sharath Ramkumar\\
Faculty Advisor: Dr. Robert Kozma\\University of Massachusetts Amherst\\COMPSCI 496: Independent Study}
\date{}

\begin{document}
\maketitle

\begin{abstract}
In this study, we explore various existing neural network models for learning sequences. We also propose alternative dynamic spiking neural architectures and compare accuracy of these network models on an artificial prediction task.
\end{abstract}

\section*{Introduction}

Neural network models are able to achieve superhuman performance on static tasks due to recent breakthroughs in deep learning. However, the majority of real-world time-series data is often complex and noisy. As a result, we need dynamic models that can adapt to changing data streams to learn and predict in an robust, semi-supervised fashion. There have been some recent studies comparing various models for time-series prediction tasks \cite{cui2016continuous} on discretized data. In this study, we will replicate their results on some of the models to establish a baseline and propose semi-surpervised spiking neural architectures for performing these tasks. Spiking neurons \cite{gerstner2002spiking} are biologically-inspired models of neurons. Spiking neural networks (SNNs) \cite{maass1997networks} are organized connections between these spiking neurons which have more energy efficient \cite{cruz2012energy} implementations on neuromorphic hardware.

\section*{Methodology}

\subsection*{Artificial Dataset}

Cui et al. (2016) propose an artificial dataset of sequences of varying length (formed with characters) with overlapping subsequences. A sequence is sampled from the dataset and presented to the model at each time step. After the last character of the sequence is presented, a character is sampled from the noise distribution and shown to the model. This process is repeated until $10,000$ characters (counting both sequences and noise characters) are shown to the model. After the model sees $10,000$ characters, the last character of sequences with shared subsequences is swapped. The performance on this task is the accuracy of predicting the last character over a window of the last $100$ sequences. A sample dataset is summarized in Table \ref{tab:dataset}.  The code and datasets are open-source and available at \href{https://github.com/sharath/sequence-learning}{https://github.com/sharath/sequence-learning}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|} \hline
        Pre-$10,000$ & Post-$10,000$  \\ \hline
        \begin{tabular}{|c|c|c|}
            Start & Subsequence & End \\ \hline
            $6$ & $8, 7, 4, 2, 3$ & $0$ \\ \hline
            $1$ & $8, 7, 4, 2, 3$ & $5$ \\ \hline
            $6$ & $3, 4, 2, 7, 8$ & $5$ \\ \hline
            $1$ & $3, 4, 2, 7, 8$ & $0$ \\ \hline
            $0$ & $9, 7, 8, 5, 3, 4$ & $1$ \\ \hline
            $2$ & $9, 7, 8, 5, 3, 4$ & $6$ \\ \hline
            $0$ & $4, 3, 5, 8, 7, 9$ & $6$ \\ \hline
            $2$ & $4, 3, 5, 8, 7, 9$ & $1$ \\ \hline
        \end{tabular} &  \begin{tabular}{|c|c|c|}
            Start & Subsequence & End \\ \hline
            $6$ & $8, 7, 4, 2, 3$ & $5$ \\ \hline
            $1$ & $8, 7, 4, 2, 3$ & $0$ \\ \hline
            $6$ & $3, 4, 2, 7, 8$ & $0$ \\ \hline
            $1$ & $3, 4, 2, 7, 8$ & $5$ \\ \hline
            $0$ & $9, 7, 8, 5, 3, 4$ & $6$ \\ \hline
            $2$ & $9, 7, 8, 5, 3, 4$ & $1$ \\ \hline
            $0$ & $4, 3, 5, 8, 7, 9$ & $1$ \\ \hline
            $2$ & $4, 3, 5, 8, 7, 9$ & $6$ \\ \hline
        \end{tabular}\\ \hline
    \end{tabular}
    \caption{A sample artificial dataset for this task. The ending character can be predicted by learning the mapping from the start character and the first character of the shared subsequence to the end character.}
    \label{tab:dataset}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{../notebooks/matrix-distances.png}
    \caption{Heatmap of squared errors between distributed random encodings for the sequence characters, with $-1$ as the separating bit for ease of visualization.}
    \label{fig:encoding-distance}
\end{figure}

\subsection*{Input Encoding}

To present each of these discrete categories to the model, we have to encode the categories to a vector representations. One possibility is to one-hot encode each character, but this representation scales poorly with the number of unique characters. For this reason, the characters are encoded as random vectors with real values in the interval $\left[-1, 1\right]$, similar to distributed representations used in natural language learning. \cite{mikolov2013distributed} The euclidean distances between pairs of unique sequence characters using a $25$ length random ending is shown in Figure \ref{fig:encoding-distance}. The decoder for converting vectors back to character uses a nearest neighbor approach, therefore the precision in the output vector from the model is important as the number of noise characters increases. 

\section*{Supervised Models}

\subsection*{Long short-term memory networks (LSTM)}

Long short-term memory \cite{hochreiter1997long} networks are able to achieve state of the art results on various sequence learning tasks, so they are a good starting point to establish baseline performance on this task. We use the same LSTM architecture proposed by Cui et al. (2016) with $25$ input neurons connected to a hidden layer of $20$ LSTM cells and $25$ output neurons. The network is trained using truncated backpropagation through time (TBPTT) \cite{mozer1995focused, sutskever2013training} on the last $100$ seen elements. The output is classified using the nearest neighbor decoder.

\subsubsection*{Long short-term memory (LSTM) Behavior}
There are many different variants of LSTM implementations. In this work, the LSTM cell has a forget gate, but no peephole connections. The equations defining the forward pass of the hidden layer in the LSTM model are:

\begin{align}
    i_t &= \sigma\left(W_{ii}x_t + W_{hi}h_{t-1} + b_i\right)\\
    f_t &= \sigma\left(W_{if}x_t + W_{hf}h_{t-1} + b_f\right)\\
    g_t &= \tanh\left(W_{ig}x_t + W_{hg}h_{t-1} + b_g\right)\\
    o_t &= \sigma\left(W_{io}x_t + W_{ho}h_{t-1} + b_o\right)\\
    c_t &= c_{t-1}\ast f_t + i_t \ast g_t\\
    h_t &= o_t \ast \tanh\left(c_t\right)
\end{align}

In the above equations, $\sigma$ is the sigmoid activation function, $i_t$ is the input gate, $f_t$ is the forget gate, $g_t$ is the cell gate, $o_t$ is the output gate and the $c_t$ and $h_t$ are the cell and hidden states respectively. The $\ast$ operator is the element-wise product operator. The biases $\left(b_i, b_f, b_g, b_o\right)$ and weights $\left(W_{ii}, W_{hi}, W_{if}, W_{hf}, W_{ig}, W_{hg},W_{io}, W_{ho}\right)$ are initialized uniformly randomly from $\left[-\frac{1}{\sqrt{k}}, \frac{1}{\sqrt{k}}\right]$, where $k$ is the number of hidden units. \cite{jozefowicz2015empirical} 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{../diagrams/lstm.png}
    \caption{The network architecture for the LSTM-online model. The LSTM cells (teal) maintain an internal hidden state.}
    \label{fig:lstm-online-model}
\end{figure}

\subsection*{Time-delay neural networks (TDNN)}

Time-delay neural networks (TDNN) are feed-forward neural networks trained using a lag on a window of previous data. \cite{waibel1995phoneme} For this task, the TDNN is trained on the last $3000$ samples every $1000$ elements with a lag of $10$. \cite{rojas1996backpropagation} The model has $250$ input neurons that are fully connected to a hidden layer of $200$ neurons. The hidden layers utilize the ReLU non-linearity and are fully connected to $25$ output neurons for the prediction. The model proposed by Cui et al. has a sigmoid non-linearity in the hidden layer. We replaced the non-linearity to allow for conversion into a spiking neural network. As with the LSTM model, the output is classified using the nearest neighbor decoder.

The network can be formulated as an equation in terms of the 250 length input vector $x_t$ and the ReLU non-linearity function $R(z)$:
\begin{equation}
    \text{TDNN}(x_t) = R\left(x_tW_{ih}^T + b_i\right)W_{ho}^T + b_h
    \label{eq:tdnn}
\end{equation}
The biases $\left(b_i, b_h\right)$ and the weights $\left(W_{ih}, W_{ho}\right)$ are initialized uniformly randomly from $\left[-\frac{1}{\sqrt{k_i}}, \frac{1}{\sqrt{k_i}}\right]$, where $k_i$ is the number of input features to the respective layer. The weights are then learned through gradient descent.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{../diagrams/tdnn.png}
    \caption{The network architecture for the TDNN/TDSNN model.}
    \label{fig:lstm-online-model}
\end{figure}

\subsection*{Time-delay spiking neural networks (TDSNN)}

A common approach to training spiking neural networks is through the transfer of weights learned through backpropagation on an identical non-spiking neural network and then scaling the weights through data-based normalization. Using this approach, we can convert the TDNN model trained with backpropagtion into an equivalent spiking network. \cite{rueckauer2017conversion}

\subsubsection*{Conversion}
The converted network has neurons that output discrete spikes rather than continuous values. Each neuron accumulates voltage $v$ from the previous layer until the voltage reaches a threshold. After the voltage surpasses the threshold, the neuron emits a discrete spike to the next layer and the voltage is reset by subtraction of the threshold. \cite{diehl2015fast} 

The change in the voltages of each of the neurons in the hidden layer at time $t$ can be written in terms of the input $x_t$, the scale for the connection between the input and hidden layer found through the data-based normalization $\lambda_{ih}$, the transferred connection and bias weights from the TDNN $\left(W_{ih}, b_{i}\right)$, and the neuron spike threshold $\tau$ as:

\begin{equation}
    \frac{dV^{\left(h\right)}_t}{dt} = \lambda_{ih}\left(W_{ih}^Tx_t + b_i\right) - S^{\left(h\right)}_{t-1}\tau
\end{equation}

where $S^{\left(h\right)}_{t}$ are the spikes in the hidden layer at time $t$. For each neuron $j$ in the hidden layer, the spike is defined as:

\begin{equation}
    S^{\left(h,j\right)}_{t} = \begin{cases} 1 & V^{\left(h,j\right)}_{t}\geq \tau \\
      0 & \text{else.} \\ \end{cases}
\end{equation}

Similarly, the following equation models the change in the voltages of the neurons in the output layer of the TDSNN:
\begin{equation}
    \frac{dV^{\left(o\right)}_t}{dt} = \lambda_{ho} \left(W_{ho}^TS^{\left(h\right)}_{t} + b_h\right) - S^{\left(o\right)}_{t-1} \tau
\end{equation}

where $\lambda_{ho}$ is the scale found through the data-based normalization for the connection between the hidden and output layer. $S^{\left(o\right)}_{t}$ is the spikes in the output layer at time $t$ which is defined for each neuron $j$ as:

\begin{equation}
    S^{\left(o,j\right)}_{t} = \begin{cases} 1 & V^{\left(o,j\right)}_{t}\geq \tau \\
      0 & \text{else.} \\ \end{cases}
\end{equation}

\subsubsection*{Original Readout}
The original readout method proposed by Rueckauer et al. looks at the voltage of the output layer rather than the spikes. This readout method combined with the the subtractive reset mechanism of the spiking neurons in the conversion method perform well on various tasks, \cite{rueckauer2017conversion} but in some cases we may not want to use the voltages. A more biologically inspired alternative for approximating the output could be the average sum of spikes over the total runtime $T$ in the output layer. This approximation of the TDNN output values can be written as:

\begin{equation}
    \text{TDSNN}_{\text{readout1}} = \frac{1}{T} \sum_{t=1}^{T}{S_t^{\left(o\right)}}
\end{equation}

\subsubsection*{Negative Readout}
When the output values produced by the TDNN is negative, the average sum of spikes fails to account for the negative output values. To account for these negative spikes, the output layer was doubled in size and the negative weight matrix of the original connections was concatenated. The same was done for the bias. The positive sum of spikes was subtracted from the negative sum of spikes and then divided by the runtime $T$ to obtain an approximation of the original network's output.

For a TDNN that has $l$ neurons in the output layer, the change in voltage for each neuron $j$ in the output layer is changed to:

\begin{equation}
    \frac{dV^{\left(o\right)}_t}{dt} = 
    \begin{cases} \lambda_{ho} \left(W_{ho}^TS^{\left(h\right)}_{t} + b_h\right) - S^{\left(o,1:l\right)}_{t-1} \tau & 1 \leq j \leq l  \\
      \lambda_{ho} \left(-W_{ho}^TS^{\left(h\right)}_{t} - b_h\right) - S^{\left(o,l+1:2l\right)}_{t-1} \tau & l+1 \leq j \leq 2l \\ \end{cases}
    \label{eq:readout}
\end{equation}

In Equation \ref{eq:readout}, $W_{ho}$ refers to the original TDNN's weight matrix and $S^{\left(o,j\right)}_{t}$ now has $j \in [1, 2l]$. The new readout is now:

\begin{equation}
    \text{TDSNN}_{\text{readout2}} = \frac{1}{T} \sum_{t=1}^{T}{\left[S_t^{\left(o,1:l\right)} - S_t^{\left(o,l+1:2l\right)}\right]}
\end{equation}

\subsubsection*{Conversion Loss}
The drawback of using spikes instead of voltages to approximate real-values is the loss of precision based on the duration of the input. For this task, we show the input for $500$ simulated milliseconds. The conversion loss, defined as the euclidean distance between the output of the TDNN and the TDSNN is shown in Figure \ref{fig:clfig}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{../results/conversion-loss.png}
    \caption{Average distance between the output of the TDNN and the TDSNN for different runtimes over the $20,000$ elements in the artificial dataset.}
    \label{fig:clfig}
\end{figure}

\section*{Semi-supervised Models}

\subsection*{K-Nearest Neighbors (KNN)}

The K-Nearest Neighbors (KNN) model makes a prediction $\hat{d}_{t+1}$ on an input $x_t$ at time $t$, where $x_t$ is the concatenation of the encodings of the characters $d_i$ for $i \in [t-9, t]$. At time $t+1$, the KNN stores the tuple $(x_t, d_{t+1})$. To make a prediction, the KNN computes the Euclidean distance from $x_t$ to all other samples. The majority vote of the top $k$ samples is taken as the prediction for $d_{t+1}$.

\subsection*{Columnar Spiking Neural Network (CSNN)}

While conversion is one way to train a spiking neural network, it is a supervised method that is still not energy efficient on hardware implementations. We propose a spiking neural network architecture, with $k$ inputs, each fully connected to $1$ of $k$ columns of $L$ Leaky Integrate-and-Fire (LIF) neurons with weights initialized uniformly randomly from $\left[0, 0.3\right]$. \cite{gerstner2002spiking} Each neuron in each column has a zero-weight connection to each neuron in each of the other columns. The inter-column connections have a Hebbian-inspired update rule. To map the spikes from this network, we use the K-Nearest Neighbors model with the sum of spikes of each neuron in each column over the runtime $T$. \cite{beliaev2007time} The model diagram is shown in Figure \ref{fig:csnn-network-diagram}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{../diagrams/csnn.png}
    \caption{The network architecture for the CSNN model.}
    \label{fig:csnn-network-diagram}
\end{figure}

\subsubsection*{Hebbian Learning Rule}
The Hebbian learning rule for the CSNN model is defined in terms of the spiking activity of its $k$ columns over the runtime $T$. The sum of spikes in a column $c$ over the runtime is defined as:

\begin{equation}
    \gamma_c = \sum_{t = 1}^{T}{s_t^{\left(c\right)}}
\end{equation}

where $s_t^{\left(c\right)}$ is a binary vector that represents the spiking neurons at time $t$ in column $c$. The mean spiking activity for each neuron $\Tilde{s}$ is then defined as:

\begin{equation}
    \Tilde{s} = \frac{1}{k} \sum_{c = 1}^{k}{\gamma_c}
    \label{eq:mean_spikes_equation}
\end{equation}

The weight change on spike in connections from column $a$ to column $b$ for learning rate $\eta$ is defined as follows:

\begin{equation}
    \Delta w_{a,b} = \eta * (\gamma_a - \Tilde{s})\otimes(\gamma_b - \Tilde{s})
\end{equation}

The weight updates for this model are computed at the end of each input rather than continuously.

\section*{Results}

\subsection*{Supervised Models}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{../results/artificial-supervised.png}
    \caption{The TDNN (teal) and TDSNN (orange) models are able to perfectly predict the sequence endings after $4,000$ elements, but their performance drops dramatically after the sequence endings are swapped. The LSTM (purple) is able to reach a reasonable performance on this task before the task and the drop after the sequences endings are swapped is only about $30\%$.}
    \label{fig:prediction-accuracy1}
\end{figure}

\subsection*{Semi-supervised Models}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{../results/artificial-unsupervised.png}
    \caption{The KNN (green) reaches perfect accuracy before the dataset is endings are swapped, but fails to relearn the data and only reaches $50\%$ accuracy afterward. The CSNN never manages to reach perfect accuracy on the task, but it is able to re-learn the sequence endings after they are swapped after experiencing a drop in performance similar to the LSTM in Figure \ref{fig:prediction-accuracy1}.}
    \label{fig:prediction-accuracy2}
\end{figure}

\section*{Conclusion}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|} \hline
        Model & Average Accuracy \\ \hline
        LSTM  & $0.8049 \pm 0.088$ \\ \hline
        TDNN  & $0.8112 \pm 0.012$ \\ \hline
        TDSNN & $0.8097 \pm 0.013$ \\ \hline
        KNN   & $0.6391 \pm 0.021$ \\ \hline
        CSNN  & $\textbf{0.8250} \pm 0.119$ \\ \hline
    \end{tabular}
    \caption{The average accuracy on the discrete sequence learning task over $10$ randomized runs.}
    \label{tab:meanaccuracy}
\end{table}

We have shown two possible spiking architectures for learning changing patterns in time-series data. We tested each of the models (LSTM, TDNN, TDSNN, KNN, CSNN) on the discrete sequence learning task.  The TDSNN (orange) is able to match the TDNN (teal) Surprisingly, the CSNN (blue) performance over $10$ runs has the highest average accuracy over the sequence learning task, but it also has the average standard deviation over the $10$ runs.

\printbibliography[title={References}]
\end{document}
